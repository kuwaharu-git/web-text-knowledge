# Webテキストナレッジ生成システム 要件定義書

## 1. プロジェクト概要

### 1.1 システム名
Webテキストナレッジ生成システム（Web Text Knowledge Generator）

### 1.2 目的
指定されたWebサイトURLから自動的にテキストコンテンツを収集し、構造化されたナレッジベースを生成するシステムを開発する。

### 1.3 対象範囲
- Webサイトのスクレイピング機能
- テキスト抽出・整形機能
- ナレッジベース生成機能
- 設定可能なクロール深度とページ数制限

## 2. 機能要件

### 2.1 基本機能

#### 2.1.1 Webスクレイピング機能
- **F001**: 指定されたURLからWebページを取得する
- **F002**: HTML構造を解析し、テキストコンテンツを抽出する
- **F003**: 画像のalt属性、title属性も含めてテキスト情報を収集する
- **F004**: リンクを辿って関連ページを取得する

#### 2.1.2 クロール制御機能
- **F005**: クロール深度を指定可能（1〜10レベル）
- **F006**: 取得ページ数の上限を設定可能（1〜1000ページ）
- **F007**: 同一ドメイン内でのクロール制限機能
- **F008**: robots.txtの遵守機能
- **F009**: リクエスト間隔の制御（礼儀正しいクロール）

#### 2.1.3 テキスト処理機能
- **F010**: HTMLタグの除去とテキスト抽出
- **F011**: 文字エンコーディングの自動判定と変換
- **F012**: 重複コンテンツの検出と除去
- **F013**: テキストの正規化（空白、改行の整理）
- **F014**: 不要なコンテンツの除去（広告、ナビゲーション等）

#### 2.1.4 ナレッジベース生成機能
- **F015**: 収集したテキストをページ単位で構造化
- **F016**: メタデータの付与（URL、タイトル、取得日時、階層レベル）
- **F017**: テキストの要約生成（オプション）
- **F018**: キーワード抽出（オプション）
- **F019**: 関連ページの紐付け情報

### 2.2 出力機能

#### 2.2.1 データ出力
- **F020**: JSON形式でのナレッジベース出力
- **F021**: CSV形式でのデータ出力
- **F022**: テキストファイル形式での出力
- **F023**: Markdown形式での出力

#### 2.2.2 レポート機能
- **F024**: スクレイピング実行結果の概要レポート
- **F025**: エラーログとスキップされたページの一覧
- **F026**: 統計情報（総ページ数、総文字数、実行時間等）

### 2.3 設定・管理機能

#### 2.3.1 設定機能
- **F027**: 設定ファイル（config.json）による動作設定
- **F028**: コマンドライン引数での設定上書き
- **F029**: 除外URL/パターンの設定機能
- **F030**: ユーザーエージェントの設定機能

#### 2.3.2 ログ機能
- **F031**: 実行ログの出力
- **F032**: エラーログの詳細記録
- **F033**: 進捗状況の表示

## 3. 非機能要件

### 3.1 性能要件
- **NF001**: 1ページあたりの処理時間は平均3秒以内
- **NF002**: 同時実行可能なリクエスト数は設定可能（デフォルト3）
- **NF003**: メモリ使用量は1GB以下で動作
- **NF004**: 1000ページのスクレイピングを2時間以内で完了

### 3.2 信頼性要件
- **NF005**: ネットワークエラーに対する自動リトライ機能（最大3回）
- **NF006**: 一時的な障害によるプロセス中断からの復旧機能
- **NF007**: 不正なHTMLに対する堅牢な処理

### 3.3 使用性要件
- **NF008**: コマンドライン実行でのシンプルな操作
- **NF009**: 設定ファイルでの詳細カスタマイズ可能
- **NF010**: 実行中の進捗状況の視覚的表示

### 3.4 セキュリティ要件
- **NF011**: 認証が必要なサイトへの対応（Basic認証）
- **NF012**: SSL/TLS証明書の検証
- **NF013**: 悪意のあるサイトからの保護

### 3.5 互換性要件
- **NF014**: Python 3.8以上での動作
- **NF015**: 主要なOS（Windows、macOS、Linux）での動作
- **NF016**: 主要なWebブラウザのUser-Agent対応

## 4. 技術要件

### 4.1 使用技術
- **言語**: Python 3.8+
- **主要ライブラリ**:
  - requests: HTTP通信
  - BeautifulSoup4: HTML解析
  - scrapy: スクレイピングフレームワーク（オプション）
  - urllib: URL処理
  - json: データ出力
  - csv: CSV出力
  - logging: ログ機能
  - concurrent.futures: 並列処理
  - time: 待機制御

### 4.2 アーキテクチャ
- **設計パターン**: MVC（Model-View-Controller）
- **モジュール構成**:
  - scraper: スクレイピング機能
  - parser: テキスト解析機能
  - knowledge_generator: ナレッジベース生成機能
  - config: 設定管理機能
  - utils: ユーティリティ機能

## 5. システム構成

### 5.1 入力仕様
```
python main.py --url <URL> --depth <深度> --max-pages <最大ページ数> [オプション]
```

**必須パラメータ**:
- `--url`: 開始URL
- `--depth`: クロール深度（1-10）
- `--max-pages`: 最大取得ページ数（1-1000）

**オプションパラメータ**:
- `--output-format`: 出力形式（json/csv/txt/md）
- `--output-file`: 出力ファイル名
- `--config`: 設定ファイルパス
- `--delay`: リクエスト間隔（秒）
- `--concurrent`: 同時リクエスト数

### 5.2 出力仕様
```json
{
  "metadata": {
    "start_url": "https://example.com",
    "crawl_depth": 3,
    "max_pages": 100,
    "execution_time": "2024-06-24T10:30:00Z",
    "total_pages": 85,
    "total_characters": 150000
  },
  "pages": [
    {
      "url": "https://example.com/page1",
      "title": "ページタイトル",
      "content": "抽出されたテキストコンテンツ",
      "metadata": {
        "fetch_time": "2024-06-24T10:30:15Z",
        "depth_level": 1,
        "character_count": 2500,
        "links": ["https://example.com/page2"]
      }
    }
  ]
}
```

## 6. 制約事項

### 6.1 技術的制約
- **C001**: JavaScript動的コンテンツは対象外（静的HTMLのみ）
- **C002**: 認証が必要なページは基本認証のみ対応
- **C003**: 大容量ファイル（画像、動画等）は処理対象外

### 6.2 法的・倫理的制約
- **C004**: robots.txtの遵守
- **C005**: 利用規約の確認が必要
- **C006**: 著作権保護コンテンツの適切な取り扱い
- **C007**: 個人情報の収集回避

### 6.3 運用制約
- **C008**: サーバーへの過度な負荷を避けるためのリクエスト制限
- **C009**: 1日あたりの実行回数制限（同一サイト）

## 7. 開発・運用要件

### 7.1 開発環境
- Python 3.8+
- 仮想環境（venv/conda）
- Git バージョン管理
- pytest テストフレームワーク

### 7.2 配布・インストール
- pip installable package
- requirements.txt による依存関係管理
- Docker対応（オプション）

### 7.3 ドキュメント
- README.md: インストール・使用方法
- API documentation
- 設定ファイル例
- トラブルシューティングガイド

## 8. テスト要件

### 8.1 単体テスト
- 各モジュールの機能テスト
- エラーハンドリングテスト
- 境界値テスト

### 8.2 統合テスト
- エンドツーエンドテスト
- 実際のWebサイトでの動作テスト
- 大量データでの性能テスト

### 8.3 テストデータ
- テスト用HTML ファイル
- モックWebサーバー
- 各種エラーケースのシミュレーション

## 9. リスク分析

### 9.1 技術的リスク
- **R001**: 対象サイトの構造変更によるスクレイピング失敗
- **R002**: 大量データ処理時のメモリ不足
- **R003**: ネットワーク不安定による処理中断

### 9.2 法的リスク
- **R004**: サイト利用規約違反
- **R005**: 著作権侵害
- **R006**: 個人情報保護法違反

### 9.3 運用リスク
- **R007**: サーバーからのアクセス制限
- **R008**: IPアドレスのブロック
- **R009**: 大量リクエストによるサーバーダウン

## 10. スケジュール

### Phase 1: 基本機能開発（2週間）
- 基本スクレイピング機能
- テキスト抽出機能
- 設定管理機能

### Phase 2: 拡張機能開発（2週間）
- 深度制御機能
- 並列処理機能
- 出力形式対応

### Phase 3: テスト・デバッグ（1週間）
- 単体テスト実装
- 統合テスト実行
- バグ修正

### Phase 4: ドキュメント・リリース（1週間）
- ドキュメント作成
- パッケージング
- リリース準備

## 11. 成果物

### 11.1 ソースコード
- main.py: メインプログラム
- scraper/: スクレイピング機能モジュール
- parser/: テキスト解析モジュール
- knowledge_generator/: ナレッジ生成モジュール
- config/: 設定管理モジュール
- tests/: テストコード

### 11.2 設定ファイル
- config.json: 基本設定
- requirements.txt: 依存関係
- setup.py: パッケージ設定

### 11.3 ドキュメント
- README.md: 使用方法
- API_DOCUMENTATION.md: API仕様
- TROUBLESHOOTING.md: トラブルシューティング
- CHANGELOG.md: 変更履歴

---

**作成日**: 2024年6月24日  
**作成者**: GitHub Copilot  
**バージョン**: 1.0